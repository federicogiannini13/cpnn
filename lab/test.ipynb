{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b2b408-1dfa-4206-9caf-b6e704da46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit this variable to set de data streams\n",
    "\n",
    "datasets = [\n",
    "    'sine_rw_10_2341'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb914743-ba84-4b85-b3d2-cd679cee3c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import statsmodels.stats.api as sms\n",
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6f1124-48a2-40d6-9d4f-fdfb8298540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa4962d-e3c3-4d5f-b68c-fcfa3bbca174",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = False\n",
    "\n",
    "models = [\"ensemble_clstm_50hs_5ws\", \"cpnn_clstm_50hs\", \"single_clstm_50hs\", \"multiple_clstm_50hs\"]\n",
    "models = [\"cpnn_clstm_50hs\", \"single_clstm_50hs\", \"multiple_clstm_50hs\"]\n",
    "\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47225311-b8b1-49aa-8196-463d5b3a3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"cpnn_clstm_50hs\": \"cPNN\",\n",
    "    \"single_clstm_50hs\": \"cLSTM\",\n",
    "    \"multiple_clstm_50hs\": \"cLSTMs\",\n",
    "    \"ensemble_clstm_50hs_5ws\": \"ensemble\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da73e8a-b9a5-4be2-90c9-2c450b552c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_inverse = {models_dict[k] : k for k in models_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50b44a55-c2c9-4f81-86fb-836335b5492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_dict = {\n",
    "    \"1\": \"SINE1+\",\n",
    "    \"2\": \"SINE2+\",\n",
    "    \"3\": \"SINE1-\",\n",
    "    \"4\": \"SINE2-\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845ac2e8-52bc-4d27-a959-d39c34fb39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(dataset, models, metric=\"accuracy\"):\n",
    "    perf_dict = {}\n",
    "    perf_interval = {}\n",
    "    for model in models:\n",
    "        try:\n",
    "            perf_dict[model] = {}\n",
    "            with open(f\"performance/{dataset}/{model}/test_then_train.pkl\", \"rb\") as f:\n",
    "                perf = pickle.load(f)\n",
    "            for k in perf:\n",
    "                perf[k] = np.asarray(perf[k])\n",
    "            for k in [metric]:\n",
    "                perf_dict[model][k] = {\n",
    "                    \"[1,50]\": np.mean(perf[k][:,:,:50], axis=2),\n",
    "                    \"[1,100]\": np.mean(perf[k][:,:,:100], axis=2),\n",
    "                    \"(100,)\": np.mean(perf[k][:,:,100:], axis=2),\n",
    "                    \"[1,)\": np.mean(perf[k], axis=2)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            del perf_dict[model]\n",
    "            print(model, \"not present\")\n",
    "    \n",
    "    return perf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5ebcb0-1db9-413b-a84e-372eb7375852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confint(perf):\n",
    "    conf = sms.DescrStatsW(perf).tconfint_mean()\n",
    "    return np.round(conf, 3), (conf[0]+conf[1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "850156c8-81b2-41ad-9219-16bf2c038315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normality(data):\n",
    "    pvalue = np.round(scipy.stats.shapiro(data).pvalue, 3)\n",
    "    if pvalue>alpha:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6b06ce-9d59-478b-a8fa-3692a8c2b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, models, metric=\"accuracy\"):\n",
    "    perf = compute_performance(dataset, models)\n",
    "    perf_batches = {}\n",
    "    tasks = dataset.split(\"_\")[-1]\n",
    "\n",
    "    for model in perf:\n",
    "        perf_batches[model] = []\n",
    "        for t in range(0,perf[model][metric][list(perf[model][metric].keys())[0]].shape[1]):\n",
    "            d = {}\n",
    "            for b in perf[model][metric]:\n",
    "                d[b] = perf[model][metric][b][:,t]\n",
    "            perf_batches[model].append(d)\n",
    "    \n",
    "    test_dfs = []\n",
    "    if len(models)==3:\n",
    "        loop = [(models[0], models[1]), (models[0], models[2]), (models[1], models[2])]\n",
    "    else:\n",
    "        loop = [models[0], models[1]]\n",
    "    for m1, m2 in loop:\n",
    "        test_df = pd.DataFrame()\n",
    "\n",
    "        for t in range(0,len(perf_batches[m1])):\n",
    "            for b in perf_batches[m2][t]:\n",
    "                mean = {}\n",
    "                row = {\n",
    "                    \"task\": tasks_dict[tasks[t]],\n",
    "                    \"batches\": b,\n",
    "                }\n",
    "                for model in [m1, m2]:\n",
    "                    c, m = calculate_confint(perf_batches[model][t][b])\n",
    "                    mean[model] = m\n",
    "                    row[models_dict[model]] = f\"{np.round(np.mean(perf_batches[model][t][b]), 3)} / {np.round(np.std(perf_batches[model][t][b]), 3)}\"\n",
    "                best = max(mean, key=mean.get)\n",
    "                worst = min(mean, key=mean.get)            \n",
    "                if check_normality(perf_batches[best][t][b]) and check_normality(perf_batches[worst][t][b]):\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_normality\"] = True\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_test\"] = \"ttest\"\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_p\"] = np.round(scipy.stats.ttest_ind(perf_batches[best][t][b], perf_batches[worst][t][b], alternative = \"greater\", equal_var=False).pvalue, 3)\n",
    "                else:\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_normality\"] = False\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_test\"] = \"wilcoxon\"\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_p\"] = np.round(scipy.stats.wilcoxon(perf_batches[best][t][b], perf_batches[worst][t][b], alternative = \"greater\").pvalue, 3)\n",
    "                if row[f\"{models_dict[m1]}-{models_dict[m2]}_p\"]<alpha:\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_GREATER\"] = models_dict[best]\n",
    "                else:\n",
    "                    row[f\"{models_dict[m1]}-{models_dict[m2]}_GREATER\"] = \"-\"\n",
    "                test_df = test_df.append(row, ignore_index=True)\n",
    "        test_dfs.append(test_df)\n",
    "    test_df_final = test_dfs[0]\n",
    "    for df in test_dfs[1:]:\n",
    "        df = df.drop(columns = [c for c in test_df_final.columns.intersection(df.columns) if c not in [\"task\", \"batches\"]])\n",
    "        test_df_final = test_df_final.merge(df, on=[\"task\", \"batches\"], how=\"inner\")\n",
    "    initial_columns = [\"task\", \"batches\"] + [models_dict[m] for m in models]\n",
    "    test_df_final = test_df_final[initial_columns + [c for c in test_df_final if c not in initial_columns]]\n",
    "    return test_df_final, perf_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92116f08-6594-4a44-9254-83fb344289c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ensemble(dataset, models, metric=\"accuracy\"):\n",
    "    perf = compute_performance(dataset, models)\n",
    "    perf_batches = {}\n",
    "    tasks = dataset.split(\"_\")[-1]\n",
    "    tasks_pos = {}\n",
    "\n",
    "    ensemble_model = [m for m in models if \"ensemble\" in m][0]\n",
    "    models = [m for m in models if \"ensemble\" not in m]\n",
    "\n",
    "    for model in perf:\n",
    "        perf_batches[model] = []\n",
    "        for t in range(0,perf[model][metric][list(perf[model][metric].keys())[0]].shape[1]):\n",
    "            d = {}\n",
    "            for b in perf[model][metric]:\n",
    "                d[b] = perf[model][metric][b][:,t]\n",
    "            perf_batches[model].append(d)\n",
    "            \n",
    "    test_df = pd.DataFrame(\n",
    "        columns=(\n",
    "            [\"task\", \"batches\"] +\n",
    "            [models_dict[m] for m in models] +\n",
    "            [\"GREATER_THAN_\"+models_dict[m] for m in models] + \n",
    "            [item for sublist in [[models_dict[m]+\"_normality\", models_dict[m]+\"_test\", models_dict[m]+\"_p\"] for m in models] for item in sublist]\n",
    "        )\n",
    "    )\n",
    "    for t in range(0,len(perf_batches[models[0]])):\n",
    "        tasks_pos[tasks_dict[tasks[t]]] = t\n",
    "        for b in perf_batches[models[0]][t]:\n",
    "            row = {\n",
    "                \"task\": tasks_dict[tasks[t]],\n",
    "                \"batches\": b,\n",
    "            }\n",
    "            for model in models+[ensemble_model]:\n",
    "                c, _ = calculate_confint(perf_batches[model][t][b])\n",
    "                row[models_dict[model]] = f\"{np.round(np.mean(perf_batches[model][t][b]), 3)} / {np.round(np.std(perf_batches[model][t][b]), 3)}\"\n",
    "            for model in models:\n",
    "                if check_normality(perf_batches[ensemble_model][t][b]) and check_normality(perf_batches[model][t][b]):\n",
    "                    row[models_dict[model]+\"_normality\"] = True\n",
    "                    row[models_dict[model]+\"_test\"] = \"ttest\"\n",
    "                    row[models_dict[model]+\"_p\"] = np.round(scipy.stats.ttest_ind(perf_batches[ensemble_model][t][b], perf_batches[model][t][b], alternative = \"greater\", equal_var=False).pvalue, 3)\n",
    "                else:\n",
    "                    row[models_dict[model]+\"_normality\"] = False\n",
    "                    row[models_dict[model]+\"_test\"] = \"wilcoxon\"\n",
    "                    row[models_dict[model]+\"_p\"] = np.round(scipy.stats.wilcoxon(perf_batches[ensemble_model][t][b], perf_batches[model][t][b], alternative = \"greater\").pvalue, 3)\n",
    "                if row[models_dict[model]+\"_p\"]<alpha:\n",
    "                    row[\"GREATER_THAN_\"+models_dict[model]] = True\n",
    "                else:\n",
    "                    row[\"GREATER_THAN_\"+models_dict[model]] = False\n",
    "            test_df = test_df.append(row, ignore_index=True)\n",
    "    initial_columns = [\"task\", \"batches\"] + [models_dict[ensemble_model]] + [models_dict[m] for m in models]\n",
    "    test_df = test_df[initial_columns + [c for c in test_df if c not in initial_columns]]\n",
    "    return test_df, perf_batches, tasks_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11fa326e-2678-41b0-a71c-9a7d46716b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sine_rw_10_2341\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not ensemble:\n",
    "    file_name = \"test\"\n",
    "    file_name_accuracy = \"accuracy\"\n",
    "    models = [m for m in models if \"ensemble\" not in m]\n",
    "else:\n",
    "    file_name = \"test_ensemble\"\n",
    "    file_name_accuracy = \"accuracy_ensemble\"\n",
    "\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    if not ensemble:\n",
    "        test_df, perf_batches = test(d, models)\n",
    "    else:\n",
    "        test_df, perf_batches, tasks_pos = test_ensemble(d, models)\n",
    "    \n",
    "    path = f\"performance/{d}/_prob_test\"\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    test_df.to_csv(os.path.join(path, f\"{file_name}_complete_{d}.csv\"), index=False)\n",
    "    test_df.to_excel(os.path.join(path, f\"{file_name}_complete_{d}.xlsx\"), index=False)\n",
    "    test_df = test_df.drop(columns = [c for c in test_df.columns if \"_normality\" in c or \"_test\" in c or \"_p\" in c])\n",
    "    test_df.to_csv(os.path.join(path, f\"{file_name}_{d}.csv\"), index=False)\n",
    "    test_df.to_excel(os.path.join(path, f\"{file_name}_{d}.xlsx\"), index=False)\n",
    "    if not ensemble:\n",
    "        test_df = test_df[[\"task\", \"batches\"] + [models_dict[m] for m in models]]\n",
    "    else:\n",
    "        for m in models:\n",
    "            if \"ensemble\" not in m:\n",
    "                test_df[models_dict[m]+\"_value\"] = test_df[models_dict[m]].apply(lambda x : float(x.split(\" / \")[0]))\n",
    "        test_df[\"best single learner\"] = test_df[[models_dict[m] for m in models if \"ensemble\" not in m]].max(axis=1)\n",
    "        test_df[\"best single learner name\"] = test_df[[models_dict[m]+\"_value\" for m in models if \"ensemble\" not in m]].idxmax(axis=1).apply(lambda x : x.replace(\"_value\", \"\"))\n",
    "        norm = []\n",
    "        for i, r in test_df.iterrows():\n",
    "            norm.append(check_normality(perf_batches[models_dict_inverse[r[\"best single learner name\"]]][tasks_pos[r[\"task\"]]][r[\"batches\"]]))\n",
    "        test_df[\"best single learner normality\"] = norm\n",
    "        test_df = test_df[[\"task\", \"batches\", \"ensemble\", \"best single learner\", \"best single learner name\", \"best single learner normality\"]]\n",
    "    test_df.to_csv(os.path.join(path, f\"{file_name_accuracy}_{d}.csv\"), index=False)\n",
    "    test_df.to_excel(os.path.join(path, f\"{file_name_accuracy}_{d}.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27878a51-7571-44fd-b32d-ce495ffb1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>batches</th>\n",
       "      <th>cPNN</th>\n",
       "      <th>cLSTM</th>\n",
       "      <th>cLSTMs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SINE2+</td>\n",
       "      <td>[1,50]</td>\n",
       "      <td>0.733 / 0.007</td>\n",
       "      <td>0.743 / 0.003</td>\n",
       "      <td>0.735 / 0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SINE2+</td>\n",
       "      <td>[1,100]</td>\n",
       "      <td>0.759 / 0.005</td>\n",
       "      <td>0.764 / 0.002</td>\n",
       "      <td>0.756 / 0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SINE2+</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.828 / 0.012</td>\n",
       "      <td>0.83 / 0.005</td>\n",
       "      <td>0.838 / 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SINE2+</td>\n",
       "      <td>[1,)</td>\n",
       "      <td>0.811 / 0.008</td>\n",
       "      <td>0.813 / 0.004</td>\n",
       "      <td>0.817 / 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SINE1-</td>\n",
       "      <td>[1,50]</td>\n",
       "      <td>0.912 / 0.017</td>\n",
       "      <td>0.88 / 0.022</td>\n",
       "      <td>0.903 / 0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SINE1-</td>\n",
       "      <td>[1,100]</td>\n",
       "      <td>0.933 / 0.015</td>\n",
       "      <td>0.926 / 0.012</td>\n",
       "      <td>0.934 / 0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SINE1-</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.972 / 0.004</td>\n",
       "      <td>0.982 / 0.001</td>\n",
       "      <td>0.975 / 0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SINE1-</td>\n",
       "      <td>[1,)</td>\n",
       "      <td>0.962 / 0.007</td>\n",
       "      <td>0.968 / 0.004</td>\n",
       "      <td>0.964 / 0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SINE2-</td>\n",
       "      <td>[1,50]</td>\n",
       "      <td>0.827 / 0.005</td>\n",
       "      <td>0.79 / 0.002</td>\n",
       "      <td>0.739 / 0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SINE2-</td>\n",
       "      <td>[1,100]</td>\n",
       "      <td>0.853 / 0.004</td>\n",
       "      <td>0.823 / 0.005</td>\n",
       "      <td>0.768 / 0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SINE2-</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.896 / 0.006</td>\n",
       "      <td>0.905 / 0.01</td>\n",
       "      <td>0.831 / 0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SINE2-</td>\n",
       "      <td>[1,)</td>\n",
       "      <td>0.885 / 0.005</td>\n",
       "      <td>0.884 / 0.009</td>\n",
       "      <td>0.815 / 0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SINE1+</td>\n",
       "      <td>[1,50]</td>\n",
       "      <td>0.954 / 0.008</td>\n",
       "      <td>0.915 / 0.01</td>\n",
       "      <td>0.918 / 0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SINE1+</td>\n",
       "      <td>[1,100]</td>\n",
       "      <td>0.962 / 0.007</td>\n",
       "      <td>0.94 / 0.006</td>\n",
       "      <td>0.943 / 0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SINE1+</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>0.981 / 0.003</td>\n",
       "      <td>0.981 / 0.001</td>\n",
       "      <td>0.976 / 0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SINE1+</td>\n",
       "      <td>[1,)</td>\n",
       "      <td>0.976 / 0.004</td>\n",
       "      <td>0.971 / 0.003</td>\n",
       "      <td>0.967 / 0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      task  batches           cPNN          cLSTM         cLSTMs\n",
       "0   SINE2+   [1,50]  0.733 / 0.007  0.743 / 0.003  0.735 / 0.002\n",
       "1   SINE2+  [1,100]  0.759 / 0.005  0.764 / 0.002    0.756 / 0.0\n",
       "2   SINE2+   (100,)  0.828 / 0.012   0.83 / 0.005  0.838 / 0.001\n",
       "3   SINE2+     [1,)  0.811 / 0.008  0.813 / 0.004  0.817 / 0.001\n",
       "4   SINE1-   [1,50]  0.912 / 0.017   0.88 / 0.022  0.903 / 0.006\n",
       "5   SINE1-  [1,100]  0.933 / 0.015  0.926 / 0.012  0.934 / 0.003\n",
       "6   SINE1-   (100,)  0.972 / 0.004  0.982 / 0.001  0.975 / 0.001\n",
       "7   SINE1-     [1,)  0.962 / 0.007  0.968 / 0.004    0.964 / 0.0\n",
       "8   SINE2-   [1,50]  0.827 / 0.005   0.79 / 0.002  0.739 / 0.004\n",
       "9   SINE2-  [1,100]  0.853 / 0.004  0.823 / 0.005  0.768 / 0.003\n",
       "10  SINE2-   (100,)  0.896 / 0.006   0.905 / 0.01  0.831 / 0.004\n",
       "11  SINE2-     [1,)  0.885 / 0.005  0.884 / 0.009  0.815 / 0.003\n",
       "12  SINE1+   [1,50]  0.954 / 0.008   0.915 / 0.01  0.918 / 0.006\n",
       "13  SINE1+  [1,100]  0.962 / 0.007   0.94 / 0.006  0.943 / 0.003\n",
       "14  SINE1+   (100,)  0.981 / 0.003  0.981 / 0.001    0.976 / 0.0\n",
       "15  SINE1+     [1,)  0.976 / 0.004  0.971 / 0.003  0.967 / 0.001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f98618-cb43-4cbe-89ac-df233546abfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
